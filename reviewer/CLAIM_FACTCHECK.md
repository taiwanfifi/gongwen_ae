# FULL_EXPLAINER.md 逐條事實查核

> **用途**：把 `FULL_EXPLAINER.md`（專案的白話解說文件）裡每一個重要說法，跟 `REVIEW_REPORT.md`（同儕審查報告）的發現對照，標出哪些說法有問題、問題在哪裡、嚴重程度多高。
>
> **寫法**：白話、工程思維、不兜圈子。

---

## 怎麼讀這份文件

每條引用格式：

> **[EXPLAINER 原文]** — 出自第 X 行
>
> **判定**：OK / 有誤導 / 有重大問題
>
> **白話說明**：……

嚴重程度用星號標示：
- ★★★ = 根本性錯誤，會誤導讀者對專案能力的理解
- ★★ = 半真半假，省略了關鍵事實
- ★ = 小問題或過度簡化

---

## 一、專案定位與 AutoEncoder 命名

### CLAIM 1 ★★★

> **原文（第 25 行）**：「把一份正式公文拆成『說了什麼』和『怎麼說的』兩個 JSON，再用這兩個 JSON 還原公文。如果還原出來的東西跟原文差不多，就證明拆法是對的。」

**判定：有重大問題**

白話說明：這句話聽起來很直覺，但有兩個坑：

1. **「差不多」是誰說的？** 還原品質的判斷有很大一部分是讓同一個 GPT-4o-mini 來打分的。它生成了公文，又自己判斷還原得好不好。這就像讓考生自己批改自己的考卷，你覺得分數會低嗎？（Review W3）

2. **「差不多」到底有多近？** 最重要的 LLM 評審指標 content_preservation，在 20 份文件裡只出現過 4 個值：0.2、0.4、0.6、0.8。它本質上是一個粗粒度的「高/低」開關，不是精確測量。說「差不多」是在用一把刻度只有 4 格的尺在量東西。（Review E2）

---

### CLAIM 2 ★★★

> **原文（第 82-125 行）**：整個「核心原理：符號化自編碼器」章節，包括架構圖、與 VAE 的對比表格。

**判定：有重大問題——核心術語誤用**

白話說明：EXPLAINER 把這個系統跟 VAE（真正的 AutoEncoder）放在同一個表格裡比較，讓讀者以為它們是同類東西的兩種版本。但它們有本質差異：

| | 真正的 AutoEncoder | 本系統 |
|---|---|---|
| Encoder 是什麼？ | 一個有參數的神經網路，通過訓練學會壓縮 | 一段 prompt，叫 LLM「幫我提取這些欄位」 |
| Decoder 是什麼？ | 一個有參數的神經網路，通過訓練學會還原 | 一段 prompt，叫 LLM「根據這些欄位寫公文」 |
| 有沒有損失函數？ | 有，重建損失（reconstruction loss）是核心 | 沒有，沒有任何東西被最小化 |
| 有沒有訓練過程？ | 有，跑幾千個 gradient descent 步驟 | 沒有，所有「能力」來自 GPT-4o-mini 的預訓練 |
| 瓶頸在哪？ | 限制潛在向量的維度（比如 128 維） | JSON schema，但 LLM 可以在裡面自由填寫任意長度的內容 |

**打個比方**：這就像你拿一本字典翻譯了一段英文，然後說「我建造了一個翻譯引擎」。字典確實能翻譯，但它不是引擎——它沒有齒輪、沒有燃油、沒有動力系統。你做的事情叫「**LLM 結構化提取 + 模板生成**」，不叫 AutoEncoder。

在 ACL/NeurIPS 等頂會，用 AutoEncoder 這個詞但系統裡沒有任何可學習參數、沒有損失函數，審稿人會直接質疑這是 buzzword borrowing。（Review W1）

---

### CLAIM 3 ★★

> **原文（第 116 行）**：「**關鍵創新**：壓縮表示不是一個不可解釋的向量，而是兩個人類可讀可編輯的 JSON。」

**判定：有誤導——不是新東西**

白話說明：用 LLM 從文件裡提取結構化 JSON，這件事在 2023-2024 年已經被大量研究了。Wei et al. (2023) 叫它「Zero-Shot Information Extraction」，Xu et al. (2024) 做了完整的 survey。「結構化提取」是 LLM 最基本的能力之一，不算創新。

真正有價值的地方是**把 Content 和 Rules 拆成兩個正交空間的框架設計**，但 EXPLAINER 強調的是 JSON 本身的可讀性，這個點不夠硬。（Review 5.4）

---

## 二、逆向資料生成

### CLAIM 4 ★★★

> **原文（第 155-161 行）**：「Step 2: 從 5 個預設模板中**隨機**選一個 Rules JSON」
>
> **原文（第 719-721 行）**：「代價：合成公文可能不如真實公文自然。但對於驗證『解耦是否成功』這個研究問題，合成數據已經足夠。」

**判定：有重大問題——隨機配對產生荒謬文件**

白話說明：EXPLAINER 輕描淡寫地說「合成公文可能不如真實公文自然」。實際情況比「不自然」嚴重得多——隨機配對產生了**語義上根本不合理**的文件：

| 文件 | 主題 | 發文機關 | 受文機關 | 荒謬在哪？ |
|------|------|---------|---------|-----------|
| 972ad03a | 校園資安防護計畫 | 經濟部工業局 | 財政部國稅局 | 工業局為什麼要向國稅局發校園資安計畫？ |
| 654b7559 | 政府採購驗收爭議 | 教育局 | 大安高中 | 教育局為什麼要向一所高中發採購爭議處理函？ |
| 9dc436b9 | 災害防救演練 | 交通部公路總局 | — | 內容提到「全校師生」——公路總局沒有「全校」 |

這不只是「不自然」，而是**語義上不連貫**。更嚴重的問題是：

**在真實世界裡，Content 和 Rules 不是正交的。** 真實公文中，什麼機關發什麼主題的文、用什麼文類，是高度相關的。「公告」幾乎只用在特定場景，「上行」語氣只出現在下級對上級。隨機配對把這些相關性全部消除了，等於是在一個**人工簡化了的、不存在的宇宙**裡做實驗。在這個宇宙裡「解耦成功」，不代表在真實世界也行。（Review W2）

---

### CLAIM 5 ★★

> **原文（第 162-163 行）**：「這就像出考試題。你先寫好『標準答案』(Content + Rules)，再根據答案出一道題（公文）。然後讓 Encoder 去『解題』，看它解出來的答案跟你的標準答案差多少。」

**判定：有誤導——省略了「出題者、解題者、閱卷者是同一人」**

白話說明：這個考試比喻聽起來很合理，但忽略了一個關鍵事實：

- **出題者** = GPT-4o-mini（生成 Content → 組合公文）
- **解題者** = GPT-4o-mini（Encode 公文 → 提取 Content/Rules）
- **閱卷者** = GPT-4o-mini（LLM Judge 打分）

同一個模型出題、解題、閱卷。已有研究（Panickssery et al. 2024）證明 LLM 會系統性偏好自己模型家族的輸出。所以整個考試的「成績」很可能被高估了。（Review W3）

---

## 三、Encoder 的解耦保證

### CLAIM 6 ★

> **原文（第 213 行）**：「Content 提取的 prompt 完全不知道 Rules 提取的存在，反之亦然。這從**架構層面保證**了『不會互相洩漏資訊』」

**判定：有誤導——prompt 隔離 ≠ 資訊隔離**

白話說明：兩個 prompt 確實是獨立發送的，但它們**讀的是同一份公文全文**。Content Encoder 可以從公文裡看到格式線索（「函」「主旨」「說明」），Rules Encoder 也可以從公文裡看到語義內容（人名、事件）。

prompt 隔離只保證了「兩個提取過程不會互相通信」，不保證「提取結果不會攜帶對方的資訊」。要真正驗證沒有資訊洩漏，需要用 mutual information 或 probing classifier 來量化。目前沒有這個驗證。（Review 9: RD7）

---

## 四、六維評估系統

### CLAIM 7 ★★

> **原文（第 428 行）**：「讓**另一個 LLM** 當『評審』」

**判定：事實錯誤——是同一個 LLM**

白話說明：EXPLAINER 用了「另一個 LLM」的措辭，讓讀者以為評審是獨立的第三方。但翻代碼看（`pipeline.py:505`, `config.py:28`），Generation、Encoding、Decoding、**Judging 全部用同一個 GPT-4o-mini**。沒有「另一個」。

這不只是措辭問題——同一個 LLM 評估自己的輸出，會有系統性的自我偏好偏差（self-enhancement bias）。已有多篇論文記錄了這個現象。（Review W3）

---

### CLAIM 8 ★★

> **原文（第 433-440 行）**：content_preservation 的 1-5 分制，暗示是細粒度的 5 級評分。

**判定：有誤導——實際上是 4 值分類器**

白話說明：EXPLAINER 畫了一個漂亮的 5 分量表（1=大量遺漏 → 5=完整保留），讓讀者以為 LLM 評審會做出精細的判斷。但實際數據顯示：

- content_preservation 在 110 個樣本裡**只出現過 4 個值**：0.2（1分）、0.4（2分）、0.6（3分）、0.8（4分）。**從來沒出現過 1.0（5分）**。
- 在 AE 路徑中只出現 {0.6, 0.8}；在 Direct 路徑中只出現 {0.2, 0.4}。

**白話翻譯**：這個「5 分制」的 LLM 評審，實際上就是一個「好/壞」的二元開關。你用它跑 paired t-test 得到 p < 0.001，數學上沒錯，但你本質上是在比較兩堆「高分」和「低分」的二元分佈。這個 p 值的意義被嚴重高估了。（Review E2）

---

### CLAIM 9 ★★

> **原文（第 484 行）**：「0.8459 代表什麼？大約 84.6% 的資訊和格式被成功保留。」

**判定：有誤導——數字的含義被過度解讀**

白話說明：weighted_total = 0.8459 不代表「84.6% 的資訊被保留」。原因：

1. **Embedding similarity 的基準線不明**：中文公文之間的 embedding cosine similarity 天然就很高（通常 > 0.7），因為它們共享大量公文用語和結構。如果隨機配對兩份不相關的公文，similarity 可能已經有 0.6-0.7。所以 0.89 的 content_accuracy 的**實際信號**可能只有 0.89 - 0.7 = 0.19，而不是 89%。
2. **JSON key 名稱的人為膨脹**：gt_content 和 predicted_content 都是 JSON 格式，key 名稱（"topic", "intent", "key_events" 等）完全相同。這些 key 在 embedding 中佔有一定比重，人為拉高了 similarity。
3. **這是一個加權合成分數**，不同指標的 0.8 含義完全不同。把它直接解讀成百分比是不準確的。（Review 6.1）

---

## 五、Paper 1：解耦實驗

### CLAIM 10 ★★★

> **原文（第 496-514 行）**：交叉重建實驗「Content_A + Rules_B → Decode → 新公文」，content_accuracy = 0.8935，結論「交叉重建成功」。

**判定：有重大問題——10 個交叉對全部來自同一份文件**

白話說明：EXPLAINER 畫了一個 A/B 交叉的漂亮示意圖，讓你以為是從多份不同文件中隨機選取的。但實際數據：

```
cross_972ad03a_526e6a63
cross_972ad03a_8c1783c5
cross_972ad03a_53ee356b
cross_972ad03a_654b7559
cross_972ad03a_d04b5ef8
...
```

**所有 10 個交叉對的 Content 都來自同一份文件 972ad03a**。

原因是 `main.py` 的嵌套迴圈有 bug：外層跑 i，內層跑 j，一旦湊滿 10 對就 break。結果 i 永遠停在 0，所以所有 Content 都來自第一份文件。

**後果**：content_accuracy 在 10 個樣本中幾乎完全相同（0.9469 ± 0.00003，標準差趨近於零），因為它們根本就是同一個 Content。

EXPLAINER 說的「content_accuracy 接近 Baseline 的 0.9004」是真的，但這只證明了**那一份特定文件**的語義在不同格式下被保留了，不能推廣到「交叉重建成功」的普遍結論。（Review E1）

---

### CLAIM 11 ★★★

> **原文（第 520-530 行）**：
> ```
> Content-only：content_accuracy = 0.9004 ← 不受影響！語義還在
> Rules-only：structural_match = 0.8889 ← 不受影響！格式還在
> ```
>
> **原文（第 665 行）**：「✅ 解耦成功」

**判定：有重大問題——這兩個數字是恆等式，不是實驗發現**

白話說明：EXPLAINER 把這兩個「不受影響」當成解耦的關鍵證據。但仔細想：

**Content-only 的 content_accuracy = Baseline 的 content_accuracy，是因為 content_accuracy 的計算方式是「gt_content vs predicted_content（Encoder 提取的 Content）」。Content-only 消融改的是 Rules，不是 Content。Encoder 提取出來的 predicted_content 完全沒變，所以 content_accuracy 當然不變。**

同理，**Rules-only 的 structural_match = Baseline 的 structural_match，因為 structural_match 比較的是「gt_rules vs predicted_rules」，Rules-only 消融沒有改 predicted_rules。**

打個比方：你拔掉電視的聲音線，然後說「畫面沒受影響」——這不是實驗發現，這是必然結果。拔聲音線當然不影響畫面。

真正需要看的消融效果在其他指標上（semantic_similarity、content_preservation、format_compliance），但那些指標的粒度很粗。所以「解耦成功」的結論，**有一半的證據是邏輯上的恆等式，不是從實驗中發現的**。（Review E5）

---

## 六、Paper 2：AE vs Direct 比較

### CLAIM 12 ★★★

> **原文（第 565 行）**：「**公平比較設計**：Direct 生成的公文也經過 Encoder 提取 Content，再跟 ground truth 比，確保兩條路徑用同一把尺評量。」

**判定：有重大問題——「同一把尺」不代表「公平」**

白話說明：EXPLAINER 強調兩條路徑用同一把尺評量，這沒錯。但它完全迴避了一個根本性的不公平——**兩條路徑拿到的輸入信息量不同**：

| 路徑 | 給 Decoder/Generator 什麼？ |
|------|---------------------------|
| **AE 路徑** | 完整的 Content JSON（topic + intent + key_events + entities + action_items + background）+ 完整的 Rules JSON |
| **Direct 路徑** | 只有一個 topic 字串 + Rules JSON |

AE 的 Decoder 拿到了「112年9月發生資安事件」「張主任」「定期檢查網路設備」等具體事實清單。Direct 的 Generator 只拿到了「校園資訊安全防護計畫」一個主題。

**這不是在比「有結構化瓶頸 vs 沒有」，是在比「給很多資訊 vs 給很少資訊」。** 如果你給 Direct Generator 同樣完整的 Content 描述（不一定要是 JSON 格式），差距很可能會大幅縮小。

用日常比喻：叫一個實習生看著詳細的事實清單寫報告（AE），跟只告訴他主題讓他自己編（Direct）——當然前者寫得好。但這不能證明「清單的 JSON 格式」有價值，只能證明「給更多資訊」有價值。（Review W4）

---

### CLAIM 13 ★★

> **原文（第 570-578 行）**：
> ```
> content_preservation: 0.7333 vs 0.3333  +120%
> 統計檢定：p = 0.000001 ***
> ```

**判定：有誤導——統計數字真實但意義被高估**

白話說明：p < 0.001 是真的，paired t-test 算出來就是這個數字。但要注意兩件事：

1. **content_preservation 在 AE 裡只有 {0.6, 0.8}，在 Direct 裡只有 {0.2, 0.4}**。兩堆數字完全不重疊。你用 t-test 比較兩個不重疊的離散分佈，當然 p 值極小。但這本質上等同於比較「一堆 3-4 分」和「一堆 1-2 分」——這是一個二元分類的結果，不是連續測量的結果。
2. **更合適的統計方法**是 Wilcoxon signed-rank test 或 McNemar's test（適用於離散/順序資料），不是 paired t-test（假設連續常態分佈）。
3. 前面說過，比較本身就不公平（CLAIM 12），所以不管 p 值多小，結論的推論基礎有問題。（Review E2）

---

### CLAIM 14 ★★

> **原文（第 549-553 行）**：
> ```
> content_similarity = 0.9772
> rules_similarity = 0.9579
> → 資訊損失極小（< 5%），AutoEncoder 是「近乎無損」的
> ```

**判定：有誤導——高循環一致性可能是 LLM 確定性模式的反映**

白話說明：循環一致性的概念是好的，但 0.97 的高分可能不是因為「資訊無損」，而是因為**同一個 GPT-4o-mini 對同樣類型的輸入會產生非常相似的輸出模式**。

打個比方：你讓同一個人翻譯一段話成英文，再翻回中文，兩次中文很像——這能說明翻譯「無損」嗎？不一定。也可能只是說明這個人有固定的翻譯風格，每次都會翻成差不多的東西。

要排除這個可能性，應該用不同的 LLM（比如 Claude 做 Encode、GPT 做 Decode、再用 Claude 做 Re-Encode），看循環一致性是否仍然高。目前沒有這個控制實驗。（Review W3）

---

## 七、Paper 3：自我修正

### CLAIM 15 ★★★

> **原文（第 633 行）**：「**Critique**（品質分析）：LLM 比較**原始公文**和重建公文，輸出結構化問題報告」

**判定：有重大問題——用了 ground truth 的修正不是「自我修正」**

白話說明：EXPLAINER 標題寫的是「Agent 能**自己**修正錯誤嗎？」但 Critique 步驟把**原始公文**（相當於正確答案）直接交給 LLM 對照。這不是「自我修正」——這是「開卷考修正」。

在真實應用中，你手上只有 Content JSON + Rules JSON → 生成的公文。你**沒有**原始公文可以對照（如果有，你就不需要重建了）。所以 Paper 3 展示的修正能力在實際部署時**用不上**。

學術文獻裡，Madaan et al. (2023) 的 Self-Refine 是真正不看答案的自我修正；Huang et al. (2024) 的研究甚至發現 LLM 在不給外部 oracle 的情況下無法有效自我修正。Paper 3 的實驗設計相當於給了 oracle，跟這兩篇的結論對不上。（Review W5）

---

### CLAIM 16 ★★

> **原文（第 679-686 行）**：「Score-gated refinement 提供了**單調改善保證**——最終分數 ≥ 初始分數，永遠不會更差。」

**判定：有誤導——這是個 trivial 的設計，不是實驗發現**

白話說明：「只保留分數更高的版本」——這當然保證不退化，這是 `max()` 函數的定義。就像說「我每次買東西只買打折的，所以我的平均消費只會越來越低」——嗯，沒錯，但這不是什麼了不起的發現。

真正該問的是：**修正有效嗎？** 答案是：3 份文件裡只有 1 份改善了（+2.3%），另外 2 份完全沒動。而且 N=3 根本無法做任何統計推斷。（Review Paper 3 評審）

---

## 八、整體架構 Q&A 區

### CLAIM 17 ★

> **原文（第 692-700 行）**：Q1 把 JSON、自然語言、向量三種方案並列比較，結論「JSON 是在結構化程度和彈性之間最好的平衡點」。

**判定：有誤導——少列了一個關鍵方案**

白話說明：這個比較沒有列出最直接的替代方案——**自由格式的 structured prompt**（例如 Markdown 格式的摘要、要點列表）。在實際 LLM 應用中，很多人用 Markdown 或自然語言要點清單做結構化中間表示，不一定要用嚴格 JSON。

另外，「數學上最優雅」用在 VAE 上是不準確的——VAE 的優雅在於它有嚴格的概率論基礎（ELBO、KL divergence），不只是「數學上好看」。（小問題）

---

### CLAIM 18 ★★

> **原文（第 718-721 行）**：「代價：合成公文可能不如真實公文自然。但對於驗證『解耦是否成功』這個研究問題，合成數據已經足夠。」

**判定：有重大誤導——問題不只是「不自然」**

白話說明：這句話暗示「合成 vs 真實」只是自然度的問題，可以接受。但真正的問題是（見 CLAIM 4）：

1. **語義不連貫**：隨機配對產生了現實中不可能存在的文件組合。
2. **人為消除了 Content-Rules 相關性**：真實世界中 Content 和 Rules 是有關聯的（什麼主題配什麼文類和語氣），但隨機配對把這種關聯消除了。在一個「Content 和 Rules 本來就無關」的數據集上驗證「Content 和 Rules 可以分開」，是自我實現的預言。
3. **N=20 且只有 10 主題 × 5 模板**：潛在空間的多樣性極度有限。

這三個問題加在一起，使得「合成數據已經足夠」這個判斷是站不住腳的。（Review W2, E3）

---

## 九、最終結論

### CLAIM 19 ★★★

> **原文（第 788 行）**：「GongWen-AE **證明了** LLM 可以充當『符號化自編碼器』——把公文拆成兩個人類可讀的 JSON 再還原，而且還原品質可量化、可控制、可自動修正。」

**判定：有重大問題——三個聲明中兩個半有問題**

逐個檢查：

| 聲明 | 判定 | 理由 |
|------|------|------|
| 「證明了 LLM 可以充當符號化自編碼器」 | 不成立 | 首先它不是 AutoEncoder（CLAIM 2）。其次「證明」在學術上需要嚴格的實驗設計，而交叉重建只用了 1 份文件的 Content（CLAIM 10），消融有恆等式問題（CLAIM 11）。頂多是「初步展示了」。 |
| 「還原品質可量化」 | 部分成立 | 確實設計了 6 維指標體系。但量化的可靠性存疑：LLM judge 粒度極低（CLAIM 8），embedding 基準線不明（CLAIM 9），同模型自評有偏差（CLAIM 7）。 |
| 「可控制」 | 部分成立 | 交叉重建的概念是好的，但只用了 1 個 Content source，無法證明泛化的可控性。 |
| 「可自動修正」 | 不成立 | 自動修正用了 ground truth（CLAIM 15），N=3 中只有 1 個改善（CLAIM 16），實際應用中無法使用。 |

---

## 總結：問題嚴重程度一覽

| 嚴重度 | 數量 | 涉及的 CLAIM |
|--------|------|-------------|
| ★★★ 根本性問題 | 7 個 | #1, #2, #4, #10, #11, #12, #15, #19 |
| ★★ 半真半假 | 7 個 | #3, #5, #7, #8, #9, #13, #14, #16, #18 |
| ★ 小問題 | 2 個 | #6, #17 |

**白話總結**：FULL_EXPLAINER.md 作為一份「白話解讀」寫得很清晰、很有結構、比喻也很生動。但它的問題不在於「寫得不清楚」，而在於**它把有根本性缺陷的實驗結果包裝成確定性的結論**。讀者看完會覺得「哇，這套系統真的 work」，但實際上：

1. **核心術語（AutoEncoder）是誤用的**
2. **最重要的實驗（交叉重建）只用了 1 份文件的 Content**
3. **消融實驗的一半證據是邏輯恆等式**
4. **AE vs Direct 的比較根本不公平**
5. **自我修正實驗偷看了答案**
6. **所有 LLM 評估都是自己評自己**

EXPLAINER 把這些問題全部省略了，只呈現了「結論」而沒有呈現「限制」。一篇誠實的解說文件應該在每個結論旁邊附上已知的 limitation。

---

*事實查核完成 — 2026-02-11*
*基於 REVIEW_REPORT.md 的同儕審查發現*
